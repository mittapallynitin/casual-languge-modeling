{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cf48cc-1a97-493f-a2be-75588c3e03e7",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183e92c3-5953-450f-9cd8-b5ffae9e7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5db7bb0c-4417-4d79-92a8-484e1b4cbd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cdb341-f338-4142-b498-b87c1d7b8305",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c019dae0-a7eb-433b-955a-b623cd3a72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_field = [\"func_documentation_string\", \"func_code_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffd53e41-3524-4ce2-b8e8-e8636c34722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f03aa38d-a276-4fa4-bff9-5acc93cedbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Remove non-ASCII characters from text.\n",
    "    \"\"\"\n",
    "    return ''.join(char for char in text if ord(char) < 128)\n",
    "    \n",
    "def clean_docstring(doc_string, max_words=20):\n",
    "    \"\"\"\n",
    "    Preprocess the documentation string:\n",
    "    - Truncate at the first empty line or limit to the first 20 words.\n",
    "    \"\"\"\n",
    "    # Split the documentation into lines\n",
    "    lines = doc_string.split(\"\\n\")\n",
    "    processed_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        # Stop if we encounter an empty line\n",
    "        if not stripped_line:\n",
    "            break\n",
    "        processed_lines.append(stripped_line)\n",
    "    return filter_non_ascii(\". \".join(processed_lines))\n",
    "\n",
    "def clean_code(code):\n",
    "    \"\"\"\n",
    "    Normalize code indentation to PEP 8 standards:\n",
    "    - Use 4 spaces per indentation level.\n",
    "    - Dynamically adjust indentation levels based on leading spaces.\n",
    "    - Skip empty lines for indentation calculations.\n",
    "    \"\"\"\n",
    "    lines = code.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    current_indent_level = 0  # Track the current indentation level\n",
    "    previous_spaces = 0  # Track the leading spaces of the last non-empty line\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.lstrip()  # Remove leading whitespace\n",
    "        leading_spaces = len(line) - len(stripped_line)  # Count leading spaces\n",
    "\n",
    "        if not stripped_line:  # If the line is empty\n",
    "            cleaned_lines.append(\"\")  # Preserve it as a blank line\n",
    "            continue  # Skip further processing for this line\n",
    "\n",
    "        # Compare leading spaces with the previous meaningful line\n",
    "        if leading_spaces > previous_spaces:\n",
    "            current_indent_level += 1  # Increase indentation level\n",
    "        elif leading_spaces < previous_spaces:\n",
    "            current_indent_level = max(0, current_indent_level - 1)  # Decrease indentation level\n",
    "\n",
    "        # Update the previous_spaces for the next comparison\n",
    "        previous_spaces = leading_spaces\n",
    "\n",
    "        # Construct the cleaned line with spaces\n",
    "        cleaned_line = (\" \" * (current_indent_level * 4)) + stripped_line\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    return filter_non_ascii(\"\\n\".join(cleaned_lines))\n",
    "    \n",
    "def preprocess_dataset(dataset):\n",
    "\n",
    "    filtered_data = []\n",
    "    for record in tqdm(dataset):\n",
    "        # Ensure both documentation and code are present\n",
    "        if record['func_documentation_string'] and record['func_code_string']:\n",
    "            filtered_data.append({\n",
    "                \"description\": clean_docstring(record['func_documentation_string']),\n",
    "                \"code\": clean_code(record['func_code_string'])\n",
    "            })\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d767239d-acad-4c8a-a537-f13a2c6b7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 412178/412178 [01:10<00:00, 5806.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = preprocess_dataset(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f810ed62-642c-4366-b187-0babb4899bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 412178\n",
      "Test Size: 23107\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Train size: {len(train_data)}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b314c91a-e1b7-4167-8273-a4ae5ae8c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def setparents(self):\n",
      "    \"\"\"Correct all parent relations for elements within the scop. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n",
      "    for c in self:\n",
      "        if isinstance(c, AbstractElement):\n",
      "            c.parent = self\n",
      "            c.setparents()\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1][\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebae07b-9239-43f9-b8dd-efcb12345249",
   "metadata": {},
   "source": [
    "# Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca4838f9-1a71-4e8a-ac18-663e2e834adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer saved at ./tokenizer/custom_tokenizer.json\n",
      "Encoded Tokens: ['<s>', 'Create', 'a', 'function', 'to', 'add', 'two', 'numbers', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import Sequence, NFKC, Strip, StripAccents\n",
    "\n",
    "def train_tokenizer(data, vocab_size=32000, save_path=\"./tokenizer\"):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer with normalization, pre-tokenization, and post-processing.\n",
    "    \"\"\"\n",
    "    # Initialize a tokenizer with a BPE model\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Step 1: Add a normalizer\n",
    "    tokenizer.normalizer = Sequence([\n",
    "        StripAccents(),  # Unicode normalization\n",
    "        Strip(),  # Remove leading/trailing spaces\n",
    "    ])\n",
    "\n",
    "    # Step 2: Add a pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "        pre_tokenizers.Whitespace(),  # Split by whitespace\n",
    "        pre_tokenizers.Punctuation(),  # Split punctuation\n",
    "    ])\n",
    "\n",
    "    # Step 3: Define a trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "    )\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer.train_from_iterator(data, trainer)\n",
    "\n",
    "    # Step 4: Add a post-processor\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<s> $A </s>\",  # For single sequences\n",
    "        pair=\"<s> $A </s> $B:1 </s>:1\",  # For paired sequences\n",
    "        special_tokens=[\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save(f\"{save_path}/custom_tokenizer.json\")\n",
    "    print(f\"Tokenizer saved at {save_path}/custom_tokenizer.json\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# Prepare the data for training the tokenizer\n",
    "data = [f\"{item['description']} {item['code']}\" for item in train_data]\n",
    "\n",
    "# Train and save the tokenizer\n",
    "custom_tokenizer = train_tokenizer(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ca8e2-125e-40d0-9ab2-cf95fa36444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer with a sample input\n",
    "sample_input = \"Create a function to add two numbers\"\n",
    "encoded = custom_tokenizer.encode(sample_input)\n",
    "print(\"Encoded Tokens:\", encoded.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
