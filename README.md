# Causal Language Modeling for Code Generation

This repository contains a **GPT-2-based causal language model** trained on the **CodeSearchNet** Python dataset. The model is designed to generate Python code given a natural language docstring as input.

## ðŸ“Œ Features

- **Custom Tokenizer**: Built from scratch instead of using the default GPT-2 tokenizer.
- **Fine-tuned GPT-2**: Trained on Python-specific code snippets.
- **Docstring-to-Code Generation**: Generates function implementations from textual descriptions.
- **Efficient Training Pipeline**: Implements dataset preprocessing, model training, and evaluation using PyTorch and Hugging Faceâ€™s Transformers.

---

## ðŸ“‚ Project Structure
causal-language-modeling/  
â”œâ”€â”€ model_config.py  
â”œâ”€â”€ train.py  
â”œâ”€â”€ tokenizer.py  
â”œâ”€â”€ preprocessor.py  
â”œâ”€â”€ data_loader.py  
â”‚â”€â”€ notebooks/  
â”‚â”€â”€ tokenizer/  
|   |â”€â”€ custom_tokenizer.json  
â”‚â”€â”€ README.md  

## Training Data
The model is trained on the Python subset of CodeSearchNet, a dataset containing 800K of function-docstring pairs.  

## ðŸ›  Technologies Used
	â€¢	PyTorch  
	â€¢	Transformers (Hugging Face)  
	â€¢	Tokenizers  
	â€¢	Datasets (Hugging Face)  

## ðŸ“œ License

This project is licensed under the MIT License. See LICENSE for details.
